# -*- coding: utf-8 -*-
"""Approccio_Proposto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JhAejFxK_-fj3lVdVYffbTNzzv5PEfjr

#Approccio Proposto
"""

#controlliamo se all'interno di una frase c'è un nome che rappresenta una persona, un gruppo, o artifatti
#https://wordnet.princeton.edu/documentation/lexnames5wn <- Lexicographer Files di wordnet
from nltk.corpus import wordnet as wn
import spacy
!python -m spacy download en_core_web_sm
!pip install liac-arff
!pip install nltk
!pip install rouge-score
!pip install pandas

nlp = spacy.load("en_core_web_sm")

import nltk
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('tagsets')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.stem import PorterStemmer
from nltk.corpus import words
from nltk import pos_tag
from nltk import RegexpParser
from sklearn.metrics import precision_score, recall_score, f1_score
import pandas as pd
from scipy.io import arff
from nltk import PCFG
import arff as liac_arff
from nltk.tokenize import word_tokenize
from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer
from nltk.translate import meteor

import re

import pandas as pd
import os
import urllib


pd.set_option('display.max_colwidth', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)


#####################START ACTORS############################################################
#############################################################################################
def identifyActors(requirementsDataframe):

  # Funzione per verificare le entità NER e restituire attori
  def effettuaNer(frase):
      list_dependencyParsingCandidati = []
      doc = nlp(frase)
      # Filtra le entità NER di tipo PERSON e ORG e aggiungile alla lista candidati
      for entity in doc.ents:
          if entity.label_ in ["PERSON", "ORG"]:
              list_dependencyParsingCandidati.append(entity.text.lower())

      return list_dependencyParsingCandidati

  requirementsDataframe["actors"] = requirementsDataframe.apply(lambda row: effettuaNer(row["requirement"]), axis=1)

  #RIMOZIONE DUPLICATI
  lista = []
  for lista_aspetto_chi in requirementsDataframe["actors"]:
      lista.append(list(dict.fromkeys(lista_aspetto_chi )))

  requirementsDataframe["actors"] = lista

  def check_PersonORgroup(word):
    excluded=["a.n.06"]
    synsets = wn.synsets(word, pos=wn.NOUN)
    for synset in synsets:
      if synset.name() in excluded:
        return False
      if is_person_or_group(synset):
          return True
    return False

  def is_person_or_group(synset):
    excluded = ["artifact", "partition", "object"]
    hypernyms = synset.hypernyms()

    for hypernym in hypernyms:
        if ('person' in hypernym.name() or 'group' in hypernym.name()) and (hypernym.name() not in excluded):
            return True
        if is_person_or_group(hypernym):
            return True
    return False

  l=[]
  for _,row in requirementsDataframe.iterrows():
    actors=row["actors"]
    temp=[]
    for actor in actors:
      for a in actor:
        if(check_PersonORgroup(a)):
          temp.append(actor)
          break
    l.append(temp[:])

  requirementsDataframe["actors"]=l
  return requirementsDataframe

#####################END ACTORS##############################################################
#############################################################################################


#####################START USE CASE##########################################################
#############################################################################################
def identifyUseCase(requirementsDataframe):
  def getCandidateWhat(dataframe):
    """https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html <-pos tags treebank
        regola per aspetto del cosa
        (<JJ|VERB>+<PRT|ADP|CONJ|PRON>*<DET>*<NOUN|ADV>+)+
    """
    patterns= """use case: {(<MD>|<JJ>|<VB>|<VBD>|<VBN>|<VBG>|<VBZ>|<VBP>|<WDT>)+(<NN>)*(<TO>|<DT>)*(<RP>|<JJ>|<TO>|<VB>|<VBD>|<VBN>|<VBG>|<VBZ>|<VBP>|<NN>|<NNS>|<NNP>|<NNPS>|<RB>|<RBR>|<RBS>|<IN>|<CD>|<DT>)+}"""
    chunker = RegexpParser(patterns)

    dataframe_list = list(dataframe['Requirement_tokenized_tagged'])
    output = [chunker.parse(row) for row in dataframe_list]


    dataframe["use case"] = output
    return dataframe

  def check_verbcognition_communication_creation(word):
    """this function given a word checks if the verb is a cognition, communication,
    creation, contact, motion, perception, possession, assist verb and returns true if so."""

    synsets = wn.synsets(word, pos=wn.VERB)
    for synset in synsets:
      if 'cognition' in synset.lexname() or 'communication' in synset.lexname() or  'creation' in synset.lexname() or 'contact' in synset.lexname() or 'motion' in synset.lexname() or 'perception' in synset.lexname() or 'possession' in synset.lexname() or "assist" in synset.lexname():
          return True
    return False

  def filterAspectOfWhatByVerbs(dataframe):
    """this function takes a dataframe and add a column: 'aspect of what' to it.
    this column contains the candidate of what in which the verb gets the value True
    from the function check_verbcognition_communication_creation."""

    lista = []
    temp = []
    for lista_aspetto_cosa in dataframe["use case"]:
      for aspetto_cosa in lista_aspetto_cosa:

        if("(use case" not in str(aspetto_cosa)):
          continue

        for c in aspetto_cosa:
          if("VB" in c[1] or "VBG" in c[1] or "VBD" in c[1] or "VBN" in c[1] or "VBN" in c[1] or "VBZ" in c[1] or "VBP" in c[1]):
            if(check_verbcognition_communication_creation(c[0])):
              temp.append(aspetto_cosa)
              break
      lista.append(temp[:])
      temp.clear()

    dataframe["use case"] = lista
    return dataframe

  requirementsDataframe=getCandidateWhat(requirementsDataframe)
  requirementsDataframe=filterAspectOfWhatByVerbs(requirementsDataframe)
  return requirementsDataframe

#####################END USE CASE############################################################
#############################################################################################


#####################START SECONDARY ACTORS##################################################
#############################################################################################
def identifySecondaryActors(requirementsDataframe):
  candidate_secondary_actors = []
  for index,row in requirementsDataframe.iterrows():
    primary_actors=row["actors"]
    if(len(primary_actors)>1):
      candidate_secondary_actors.append(primary_actors[1:])
    else:
      candidate_secondary_actors.append([])
  requirementsDataframe["secondary_actors"]=candidate_secondary_actors
  return requirementsDataframe
#####################END SECONDARY ACTORS####################################################
#############################################################################################

#####################START DESCRIPTION#######################################################
#############################################################################################
def generateDescription(requirementsDataframe):
  description=[]
  descriptionCleaned=""
  pattern_final = r'/[A-Z]+\b'
  pattern_intermediate = r'(/\b[A-Z]+\b)(?=.*(/\b[A-Z]+\b))'

  for index,row in requirementsDataframe.iterrows():
    actor=row["actors"]
    use_case=row["use case"]
    temp=[]
    for a,u in zip(actor,use_case) :
      actorCleaned = str(a).replace("\n","")
      actorCleaned = actorCleaned.replace("(candidate actor", "").replace(")","").replace("(" ,"")
      actorCleaned = re.sub(pattern_intermediate, '', actorCleaned)
      actorCleaned = re.sub(pattern_final, '', actorCleaned)

      use_caseCleaned = str(u).replace("\n","")
      use_caseCleaned = use_caseCleaned.replace("(use case", "").replace(")","").replace("(" ,"")
      use_caseCleaned = re.sub(pattern_intermediate, '',  use_caseCleaned)
      use_caseCleaned = re.sub(pattern_final, '',  use_caseCleaned)


      temp.append("The UC describe the functionality that allows " + actorCleaned + " " + use_caseCleaned )

      actorCleaned=""
      use_caseCleaned=""

    description.append(temp)

  requirementsDataframe["description"]=description
  return requirementsDataframe
#####################END DESCRIPTION#########################################################
#############################################################################################


####################GENERATION#############################################################
############################################################################################
def generateActors_AndUseCase_AndSecondaryActors_AndDescription(requirementsDataframe):
  """this function takes a dataframe in input, if dataframe is None then
  you can specify the datasetPath to an url path to retrieve the dataset
  from interntet.(Most likely you' ll specify the raw url of dataset on github)
  Once we have the dataset the function will return a list of user stories from the dataset
  specified.
  :requirementsDataframe a dataframe
  :return a list of use_case from the dataset specified
  """

  #tokenizzazione, verrà aggiunta l colonna Requirement_tokenized che dice che i requisiti sono stati tokenizzati
  requirementsDataframe['Requirement_tokenized'] = requirementsDataframe.apply(lambda row: nltk.word_tokenize(row['requirement']), axis=1)
  requirementsDataframe = requirementsDataframe.reset_index(drop=False)

  #pos tagging, verrà aggiunta l colonna Requirement_tokenized_tagged che dice che i requisiti tokenizzati sono stati taggati con le POS
  requirementsDataframe['Requirement_tokenized_tagged'] = requirementsDataframe['Requirement_tokenized'].apply(lambda row: pos_tag(row))

  requirementsDataframe = identifyActors(requirementsDataframe)
  requirementsDataframe = identifyUseCase(requirementsDataframe)
  requirementsDataframe = identifySecondaryActors(requirementsDataframe)
  requirementsDataframe = generateDescription(requirementsDataframe)
  return requirementsDataframe

####################GENERATION########################################################
######################################################################################


################## START ## MAIN ###########################################################
############################################################################################
requirementsURL = "https://raw.githubusercontent.com/gianlucatrani/userStoriesGenerator/refs/heads/main/dataset/PROMISE_exp.arff"
#crea la cartella
os.makedirs("dataset", exist_ok=True)
file_path = os.path.join("dataset","PROMISE_exp.arff")
urllib.request.urlretrieve(requirementsURL, file_path)
datasetPath = os.path.join("dataset","PROMISE_exp.arff")

# Read the file content
with open(datasetPath, 'r') as f:
    file_content = f.read()

# Replace the problematic escape sequence
file_content = file_content.replace('\\92', '\\\\92')

# Load the modified content using liac_arff
arff_file = liac_arff.loads(file_content)

data = arff_file['data'] # Extract the data
df = pd.DataFrame(columns=["document", "requirement", "class of Requirement"], data = data)

#terzo_doc=df[df["document"]=="3"]
#terzo_doc=terzo_doc[terzo_doc["class of Requirement"]=="F"]

terzo_doc=df[df["document"]=="49"]
terzo_doc=terzo_doc[terzo_doc["class of Requirement"]=="F"]
for index,row in terzo_doc.iterrows():
  if("Program Administrators and " in row["requirement"]):
    row["requirement"]=row["requirement"].replace("Program Administrators and ","Program Administrators/")

terzo_doc = generateActors_AndUseCase_AndSecondaryActors_AndDescription(terzo_doc)

#print(terzo_doc)

################## END ## MAIN ###########################################################
############################################################################################


####################EVALUATION##############################################################
############################################################################################

#-----------------------------------Valutazione degli attori tramite Chat GPT----------------------------

#listaAttoriIndividuati
listaAttoriIndividuati=[]
# Lista dei POS tags per i possibili nomi e sostantivi
for index,row in terzo_doc.iterrows():
  for i in range(len(row["actors"])):
   p=str(row['actors'][i])
   p=p.replace("(candidate actor", "").replace(")","").replace("(", "").replace("\n","")
   pattern_intermediate = r'(/\b[A-Z]+\b)(?=.*(/\b[A-Z]+\b))'
   pCleaned = re.sub(pattern_intermediate, '', p)
   pCleaned = re.sub(r'\s+', ' ', pCleaned)

   pattern_final = r'/[A-Z]+\b'
   pCleaned = re.sub(pattern_final, '', pCleaned)
   listaAttoriIndividuati.append(pCleaned)
   listaAttoriIndividuati = [p.strip().lower() for p in listaAttoriIndividuati]
listaAttoriIndividuati=list(dict.fromkeys(listaAttoriIndividuati))
mapping = {attore: idx + 1 for idx, attore in enumerate(listaAttoriIndividuati)}
lista_2_encoded = [mapping[attore] for attore in listaAttoriIndividuati]
print(listaAttoriIndividuati)
print(lista_2_encoded)

#importing dell'oracolo
print("Valutazione degli attori tramite oracolo CHAT GPT".upper())

#del terzo doc
#oracoloGPT = "https://raw.githubusercontent.com/gianlucatrani/userStoriesGenerator/refs/heads/main/oracolo/oracolo_chatgpt_use_cases.csv"
#oracoloGPT="https://raw.githubusercontent.com/gianlucatrani/userStoriesGenerator/refs/heads/main/oracolo/oracolo_chatgpt_use_cases_num9.csv"
oracoloGPT="https://raw.githubusercontent.com/gianlucatrani/userStoriesGenerator/refs/heads/main/oracolo/oracolo_chatgpt_use_cases_49.csv"
#crea la cartella
os.makedirs("oracolo", exist_ok=True)
oracoloGPTPath = os.path.join("oracolo","oracolo_chatgpt_use_cases_49.csv")
urllib.request.urlretrieve(oracoloGPT, oracoloGPTPath )
dataFrame_OracoloChatGpt=pd.read_csv(oracoloGPTPath)
listaAttoriOracolo = dataFrame_OracoloChatGpt['Primary Actor']
listaAttoriOracolo = list((listaAttoriOracolo))
listaAttoriOracolo = list(dict.fromkeys(listaAttoriOracolo))
mapping = {attore: idx + 1 for idx, attore in enumerate(listaAttoriOracolo)}
lista_1_encoded = [mapping[attore] for attore in listaAttoriOracolo]
print(listaAttoriOracolo)
print(lista_1_encoded)

#calcolo metriche
listaAttoriIndividuati_normalized = [attore.strip().lower() for attore in listaAttoriIndividuati]
listaAttoriOracolo_normalized = [attore.strip().lower() for attore in listaAttoriOracolo]

attori_unici = set(listaAttoriIndividuati_normalized).union(set(listaAttoriOracolo_normalized))
y_true = [1 if attore in listaAttoriOracolo_normalized else 0 for attore in attori_unici]
y_pred = [1 if attore in listaAttoriIndividuati_normalized else 0 for attore in attori_unici]


from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print(" CALCOLO METRICHE: \n ")

print(f"Precisione: {precision:.2f}")
print(f"Richiamo (Recall): {recall:.2f}")
print(f"F1-Score: {f1:.2f}")


#-----------------------------------Valutazione delle descrizioni tramite Chat GPT----------------------------
print("oracolo descrizioni tramite ChatGPT".upper() + "\n")
grandeStringaDescrizioneOracolo = dataFrame_OracoloChatGpt.groupby('Primary Actor')['Description'].apply(lambda x: ' '.join(x)).to_dict()
print(" DESCRIZIONE PER ATTORE: \n ")

for attore, descrizione in grandeStringaDescrizioneOracolo.items():
    print(f"Attore: {attore}\nDescrizione: {descrizione}\n")

#descrizione grande stringa per attore individuato
descrizioni_consolidate = {attore: "" for attore in listaAttoriIndividuati}
for attore in listaAttoriIndividuati:
    descrizioni = []
    for index, row in terzo_doc.iterrows():
        for descrizione in row["description"]:
            if attore in descrizione.lower():
                descrizioni.append(descrizione)
    descrizione_concatenata = " ".join(descrizioni)
    descrizioni_consolidate[attore] = descrizione_concatenata
#for attore, descrizione in descrizioni_consolidate.items():
#    print(f"Attore: {attore}\nDescrizione: {descrizione}\n")

#calcolo metriche
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
f = open(oracoloGPTPath, "r")
reference = descrizioni_consolidate
candidate = grandeStringaDescrizioneOracolo
reference_text = ' '.join(reference.values())
candidate_text = ' '.join(candidate.values())


print(" CALCOLO METRICHE: \n ")

scores = scorer.score(reference_text, candidate_text)
print(f"ROUGE Scores: {scores}")

bleu_score = sentence_bleu([reference_text.split()], candidate_text.split())
print(f"BLEU Score: {bleu_score:.4f}")


meteor_score = meteor([reference_text.split()], candidate_text.split())
print(f"METEOR Score: {meteor_score:.4f}")
print()
f.close()

#-----------------------------------Valutazione degli attori tramite oracolo manuale-------------------------

print("\n Valutazione degli attori tramite oracolo manuale".upper() + "\n")
#del terzo doc
#oracoloManuale = "https://raw.githubusercontent.com/gianlucatrani/userStoriesGenerator/main/oracolo/oracolo_manuale_3.csv"
#oracoloManuale="https://raw.githubusercontent.com/gianlucatrani/userStoriesGenerator/refs/heads/main/oracolo/oracolo_manuale_num9.csv"
oracoloManuale="https://raw.githubusercontent.com/gianlucatrani/userStoriesGenerator/refs/heads/main/oracolo/oracolo_manuale_49.csv"
os.makedirs("oracolo", exist_ok=True)
#oracoloManualePath = os.path.join("oracolo", "oracolo_manuale_3.csv")
#oracoloManualePath = os.path.join("oracolo", "oracolo_manuale_num9.csv")
oracoloManualePath = os.path.join("oracolo", "oracolo_manuale_49.csv")
urllib.request.urlretrieve(oracoloManuale, oracoloManualePath)
#dataFrame_OracoloManuale = pd.read_csv(oracoloManualePath , sep='\t') per il 9
dataFrame_OracoloManuale = pd.read_csv(oracoloManualePath)
#listaAttoriOracoloManuale = dataFrame_OracoloManuale['PRIMARY ACTOR']
listaAttoriOracoloManuale = dataFrame_OracoloManuale['Primary Actor']
listaAttoriOracoloManuale = list((listaAttoriOracoloManuale))
listaAttoriOracoloManuale = list(dict.fromkeys(listaAttoriOracoloManuale))
mapping = {attore: idx + 1 for idx, attore in enumerate(listaAttoriOracoloManuale)}
lista_3_encoded = [mapping[attore] for attore in listaAttoriOracoloManuale]
print(listaAttoriOracoloManuale)
print(lista_3_encoded)

def normalize_actor(actor):
    return actor.strip().lower()

listaAttoriIndividuati_normalized = [normalize_actor(attore) for attore in listaAttoriIndividuati]
listaAttoriOracoloManuale_normalized = [normalize_actor(attore) for attore in listaAttoriOracoloManuale]

attori_unici_1 = set(listaAttoriIndividuati_normalized).union(set(listaAttoriOracoloManuale_normalized))
y_true = [1 if attore in listaAttoriOracoloManuale_normalized else 0 for attore in attori_unici_1]
y_pred = [1 if attore in listaAttoriIndividuati_normalized else 0 for attore in attori_unici_1]



precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print(f"Precisione: {precision:.2f}")
print(f"Richiamo (Recall): {recall:.2f}")
print(f"F1-Score: {f1:.2f}")




#-----------------------------------Valutazione degli descrizioni tramite oracolo manuale--------------------

print("\n Valutazione degli descrizioni tramite oracolo manuale".upper() + "\n")
#grandeStringaDescrizioneOracoloManuale =dataFrame_OracoloManuale.groupby('PRIMARY ACTOR')['DESCRIPTION'].apply(lambda x: ' '.join(x)).to_dict()
grandeStringaDescrizioneOracoloManuale =dataFrame_OracoloManuale.groupby('Primary Actor')['Description'].apply(lambda x: ' '.join(x)).to_dict()
for attore, descrizione in grandeStringaDescrizioneOracoloManuale.items():
    print(f"Attore: {attore}\nDescrizione: {descrizione}\n")


scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
f = open(oracoloGPTPath, "r")

reference = descrizioni_consolidate
candidateManuale = grandeStringaDescrizioneOracoloManuale

reference_text = ' '.join(reference.values())
candidate_textM = ' '.join(candidateManuale.values())

scores = scorer.score(reference_text, candidate_textM)
print(f"ROUGE Scores: {scores}")

bleu_score = sentence_bleu([reference_text.split()], candidate_textM.split())
print(f"BLEU Score: {bleu_score:.4f}")

meteor_score = meteor([reference_text.split()], candidate_textM.split())
print(f"METEOR Score: {meteor_score:.4f}")
print()
f.close()

print()
print()


####################END EVALUATION##############################################################
############################################################################################